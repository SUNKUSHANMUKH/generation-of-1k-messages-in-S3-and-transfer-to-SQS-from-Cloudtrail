# CloudTrail â†’ SNS â†’ SQS â†’ Fluentd â†’ Node.js â†’ SQLite Pipeline

This project demonstrates an endâ€‘toâ€‘end log ingestion pipeline:

AWS CloudTrail  
â†’ S3  
â†’ SNS
â†’ SQS  
â†’ Fluentd (running in Docker)  
â†’ Node.js app (REST API)  
â†’ SQLite database

The goal is to collect CloudTrail/S3 event notifications and store them in a local SQLite DB for testing, analytics, or building downstream processors.

---

## ğŸš€ Architecture Overview

1. **CloudTrail** writes logs to an S3 bucket.  
2. **S3 Event Notification** sends events to the **SQS queue**.  
3. **Fluentd** reads messages from SQS using `fluent-plugin-sqs`.  
4. Fluentd parses SNS/S3/CloudTrail JSON and POSTs the full log to:  
   ```
   POST /ingest
   ```
   on the Node.js application.
5. The Node.js service writes events into a SQLite database  
   (`cloudtrail.db` inside `app/data/`).

---

## ğŸ“ Project Structure

```
cloudtrail-project/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ log-generator.sh         # optional load generator
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ server.js
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ Dockerfile
â””â”€â”€ fluentd/
    â”œâ”€â”€ fluent.conf
    â””â”€â”€ Dockerfile
```

---

## ğŸ§© Components

### 1. Node.js App
Runs an Express server that exposes `/ingest` and writes logs into SQLite:

- Creates DB file at `/usr/src/app/data/cloudtrail.db`
- Inserts every incoming event
- Logs requests to console for easy debugging

### 2. Fluentd
Reads messages from your AWS SQS queue using:

```
fluent-plugin-sqs
fluent-plugin-out-http
```

It parses:

- SQS body â†’ SNS  
- SNS Message â†’ CloudTrail/S3 event JSON  

Then forwards to the Node app.

---

## ğŸ³ Running with Docker Compose

### 1. Export AWS credentials:

```
export AWS_ACCESS_KEY_ID="YOUR_KEY"
export AWS_SECRET_ACCESS_KEY="YOUR_SECRET"
```

### 2. Start the stack:

```
docker-compose up -d --build
```

### 3. Check logs:

```
docker logs app --tail 20
docker logs fluentd --tail 20
```

You should see logs entering the DB.

---

## ğŸ” Testing the Pipeline

Upload a file to your CloudTrail S3 bucket:

```
echo '{"msg":"pipeline test"}' > test.json
aws s3 cp test.json s3://YOUR-CLOUDTRAIL-BUCKET/
```

Wait 5â€“10 seconds, then check SQLite:

```
sqlite3 app/data/cloudtrail.db "SELECT COUNT(*) FROM logs;"
```

If everything works, the number increases with every S3 upload.

---

## âš™ï¸ Optional: Log Generator

A small script that uploads 1 file per second:

```
./log-generator.sh
```

---

## ğŸ§¹ Clean Up

Stop containers:

```
docker-compose down
```

Remove volumes (including SQLite DB):

```
docker-compose down -v
```

---

## ğŸ™Œ Notes

- You may replace SQLite with PostgreSQL if moving to production.  
- Docker Compose allows scaling the app service for parallel inserts.  
- Fluentd config can be extended for transformations, filtering, or routing.

---

## ğŸ’¬ Need Improvements?

I can also generate:

- Architecture diagram  
- Terraform / CloudFormation  
- A TypeScript version  
- A production-ready version with PostgreSQL, retries, and metrics  

Just ask!


---

# ğŸ—ï¸ A) AWS Manual Configuration Steps

Below are the exact steps to manually configure AWS so S3 â SNS â SQS â Fluentd pipeline works endâ€‘toâ€‘end.

---

## **Stepâ€‘1: Create an S3 Bucket**
1ï¸âƒ£ Go to **AWS Console â†’ S3**  
2ï¸âƒ£ Click **Create bucket**  
3ï¸âƒ£ Example bucket name:

```
aws-cloudtrail-logs-<ACCOUNT-ID>-project
```

4ï¸âƒ£ Region: **ap-south-1**  
5ï¸âƒ£ Block Public Access: **Enabled**  
6ï¸âƒ£ Click **Create bucket**

---

## **Stepâ€‘2: Enable CloudTrail & Send Logs to S3**
1ï¸âƒ£ Go to **CloudTrail â†’ Trails**  
2ï¸âƒ£ Click **Create trail**

- **Name:** `cloudtrail-project-trail`  
- Enable **Management** & **Data** events  
- Select the S3 bucket created above  

Click **Create trail** ğŸš€

---

## **Stepâ€‘3: Create SNS Topic**
1ï¸âƒ£ Go to **SNS â†’ Topics â†’ Create topic**  
2ï¸âƒ£ Choose **Standard**  
3ï¸âƒ£ Name:

```
cloudtrail-log-topic
```

Click **Create Topic** âœ”ï¸

---

## **Stepâ€‘4: Create SQS Queue**
1ï¸âƒ£ Go to **SQS â†’ Create Queue**  
2ï¸âƒ£ Name:

```
1k-SQS-Notification
```

3ï¸âƒ£ Type: **Standard Queue**  
4ï¸âƒ£ Leave all defaults â†’ **Create** âœ”ï¸

---

## **Stepâ€‘5: Subscribe SQS to SNS**
1ï¸âƒ£ Go to **SNS** â†’ open topic **cloudtrail-log-topic**  
2ï¸âƒ£ Click **Create subscription**  
3ï¸âƒ£ Set:

- **Protocol:** Amazon SQS  
- **Endpoint:** select `1k-SQS-Notification`

Click **Create Subscription**

---

## **Stepâ€‘6: Allow SNS to Send Messages to SQS**
1ï¸âƒ£ Go to **SQS â†’ 1k-SQS-Notification â†’ Permissions â†’ Access Policy â†’ Edit**  
2ï¸âƒ£ Add this:

```json
{
  "Version": "2012-10-17",
  "Statement": [{
    "Effect": "Allow",
    "Principal": {"Service": "sns.amazonaws.com"},
    "Action": "sqs:SendMessage",
    "Resource": "arn:aws:sqs:ap-south-1:<ACCOUNT-ID>:1k-SQS-Notification",
    "Condition": {
      "ArnEquals": {
        "aws:SourceArn": "arn:aws:sns:ap-south-1:<ACCOUNT-ID>:cloudtrail-log-topic"
      }
    }
  }]
}
```

Replace `<ACCOUNT-ID>` with your AWS account ID.

---

## **Stepâ€‘7: Enable S3 Event Notifications**
1ï¸âƒ£ Go to **S3 â†’ Bucket â†’ Properties**  
2ï¸âƒ£ Scroll to **Event notifications â†’ Create**  
3ï¸âƒ£ Configure:

- **Name:** `SendS3ToSNS`  
- **Event type:** PUT  
- **Destination:** SNS  
- Select: **cloudtrail-log-topic**  

Click **Save** âœ”ï¸

---

### ğŸ¯ Final Flow

```
S3 (PUT event)
   â†“
SNS Topic
   â†“
SQS Queue
   â†“
Fluentd Container
   â†“
Node.js API (/ingest)
   â†“
SQLite Database
```

The pipeline is now fully active!  
